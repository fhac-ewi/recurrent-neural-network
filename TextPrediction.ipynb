{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187b5fa7",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fhac-ewi/recurrent-neural-network/blob/Textprediction/TextPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5861673e-6e74-48a9-9751-e72cb8ba0524",
   "metadata": {
    "id": "5861673e-6e74-48a9-9751-e72cb8ba0524"
   },
   "source": [
    "Textvervollständigung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56798713-034f-4a5e-abaa-bdc926e15813",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56798713-034f-4a5e-abaa-bdc926e15813",
    "outputId": "05e647b1-93e3-4372-f9a0-4a504c06942f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Version: 2.4.3 ; Tensorflow version: 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "print(\"Keras Version:\", keras.__version__, \"; Tensorflow version:\", tf.__version__)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc468458-e438-4c9b-98b4-26d6f1948fbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc468458-e438-4c9b-98b4-26d6f1948fbc",
    "outputId": "86cd0a39-9fdc-4014-b3b5-4e46e6b49769"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaiable tensorflow devices\n",
      " -> /device:CPU:0 is a CPU (  )\n",
      " -> /device:XLA_CPU:0 is a XLA_CPU ( device: XLA_CPU device )\n",
      " -> /device:XLA_GPU:0 is a XLA_GPU ( device: XLA_GPU device )\n",
      " -> /device:GPU:0 is a GPU ( device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1 )\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(\"Avaiable tensorflow devices\")\n",
    "for d in device_lib.list_local_devices():\n",
    "    print(\" ->\", d.name, \"is a\", d.device_type, \"(\", d.physical_device_desc, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "oVsucpBjABiX",
   "metadata": {
    "id": "oVsucpBjABiX"
   },
   "outputs": [],
   "source": [
    "def generate_heading(title):\n",
    "    title = str(title).upper()\n",
    "    return \"\\n\".join((f\"--{title}--\", \"_\" * (len(title) + 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eYsKNKdZW5t",
   "metadata": {
    "id": "8eYsKNKdZW5t"
   },
   "outputs": [],
   "source": [
    "def clean_book(txt):\n",
    "    started = False\n",
    "    after_page = False\n",
    "    ret_txt = []\n",
    "    for line in txt.split(\"\\n\"):\n",
    "        if started:\n",
    "            if after_page:\n",
    "                if len(line.strip()) == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    after_page = False\n",
    "                    ret_txt.append(line)\n",
    "            else:\n",
    "                if line.startswith(\"Page | \") or line.strip() == \"*\":\n",
    "                    after_page = True\n",
    "                else:\n",
    "                    ret_txt.append(line)\n",
    "        else:\n",
    "            if line.startswith(\"/\") or line.startswith(\"I\"):\n",
    "                started = True\n",
    "    return (\"\\n\".join(ret_txt)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Pr9E3_TNdc4J",
   "metadata": {
    "id": "Pr9E3_TNdc4J"
   },
   "outputs": [],
   "source": [
    "def save_book(idx, title, text):\n",
    "    open(f\"./{idx} - {title}.txt\", 'wb').write((\"\\n\\n\\n\\n\".join((generate_heading(title), text))).encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f6c56bd-f7a9-417f-8632-b67a418ae241",
   "metadata": {
    "id": "0f6c56bd-f7a9-417f-8632-b67a418ae241"
   },
   "outputs": [],
   "source": [
    "books = [\n",
    "    (\"Harry Potter and the Philosophers stone\", \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%201%20-%20The%20Philosopher's%20Stone.txt\"),\n",
    "    (\"Harry Potter ańd the Chamber Of Secrets\", \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%202%20-%20The%20Chamber%20of%20Secrets.txt\"),\n",
    "    (\"Harry Potter and the Prisoner Of Azkaban\", \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%203%20-%20The%20Prisoner%20of%20Azkaban.txt\"),\n",
    "    (\"Harry Potter and the Goblet Of Fire\", \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%204%20-%20The%20Goblet%20of%20Fire.txt\"),\n",
    "    (\"Harry Potter and the Order Of The Phoenix\", \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%205%20-%20The%20Order%20of%20the%20Phoenix.txt\"),\n",
    "    (\"Harry Potter and the Half Blood Prince\", \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%206%20-%20The%20Half%20Blood%20Prince.txt\"),\n",
    "    (\"Harry Potter and the Deathly Hallows\", \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%207%20-%20The%20Deathly%20Hallows.txt\")\n",
    "]\n",
    "\n",
    "downloaded_files = {}\n",
    "for i, (name, url) in enumerate(books):\n",
    "    downloaded_files[(i, name)] = keras.utils.get_file(f\"training_text_{i}.txt\", url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4lyY6btYacq5",
   "metadata": {
    "id": "4lyY6btYacq5"
   },
   "outputs": [],
   "source": [
    "processed_texts = {}\n",
    "\n",
    "for (i, name), path in downloaded_files.items():\n",
    "    text_load = open(path, 'rb').read().decode(encoding='utf-8')\n",
    "    processed_texts[name] = clean_book(text_load)\n",
    "    save_book(i + 1, name, processed_texts[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21a357dd-8c00-43a0-82c0-c7639d531682",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21a357dd-8c00-43a0-82c0-c7639d531682",
    "outputId": "ac8bf32b-cb6d-4935-eef1-0b7ee37cc90b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 6461583 characters\n"
     ]
    }
   ],
   "source": [
    "text = \"\\n\\n\".join(processed_texts.values())\n",
    "\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "blD593enkuNs",
   "metadata": {
    "id": "blD593enkuNs"
   },
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84aa973f-fbe6-4fc5-be33-0810b389d87e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84aa973f-fbe6-4fc5-be33-0810b389d87e",
    "outputId": "184100f8-1f71-4283-a312-61528c822180"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df64822d-7879-4afb-b699-78017d35f18b",
   "metadata": {
    "id": "df64822d-7879-4afb-b699-78017d35f18b"
   },
   "outputs": [],
   "source": [
    "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
    "chars_from_ids = keras.layers.experimental.preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77705e32-6f36-4089-bd0d-61bd3dd35f5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77705e32-6f36-4089-bd0d-61bd3dd35f5e",
    "outputId": "41824ffd-9171-498d-9da2-b81bae8e89c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6461583,), dtype=int64, numpy=array([48, 36, 33, ..., 68, 68, 13])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52HTw6Vtkqas",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52HTw6Vtkqas",
    "outputId": "27b82aba-bece-45a3-a346-e2ba2f283e04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int64>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "ids_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ym90kfqlkqiZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ym90kfqlkqiZ",
    "outputId": "955eb052-dea5-4cb9-ce3f-8d27c4fbb81d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n",
      "H\n",
      "E\n",
      " \n",
      "B\n",
      "O\n",
      "Y\n",
      " \n",
      "W\n",
      "H\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "U--NQ5L-lj3m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U--NQ5L-lj3m",
    "outputId": "dcd479e2-a6c0-4fd3-8f7c-4ef90a2fd06b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'T' b'H' b'E' b' ' b'B' b'O' b'Y' b' ' b'W' b'H' b'O' b' ' b'L' b'I'\n",
      " b'V' b'E' b'D' b' ' b'\\n' b'\\n' b'M' b'r' b'.' b' ' b'a' b'n' b'd' b' '\n",
      " b'M' b'r' b's' b'.' b' ' b'D' b'u' b'r' b's' b'l' b'e' b'y' b',' b' '\n",
      " b'o' b'f' b' ' b'n' b'u' b'm' b'b' b'e' b'r' b' ' b'f' b'o' b'u' b'r'\n",
      " b',' b' ' b'P' b'r' b'i' b'v' b'e' b't' b' ' b'D' b'r' b'i' b'v' b'e'\n",
      " b',' b' ' b'\\n' b'w' b'e' b'r' b'e' b' ' b'p' b'r' b'o' b'u' b'd' b' '\n",
      " b't' b'o' b' ' b's' b'a' b'y' b' ' b't' b'h' b'a' b't' b' ' b't' b'h'\n",
      " b'e' b'y' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fKz-1qJlpQ0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2fKz-1qJlpQ0",
    "outputId": "72c79a7e-7a70-4498-d011-3d3bb35af99a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'THE BOY WHO LIVED \\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, \\nwere proud to say that they '\n",
      "b'were perfectly normal, \\nthank you very much. They were the last people you\\xe2\\x80\\x99d \\nexpect to be involved i'\n",
      "b'n anything strange or \\nmysterious, because they just didn\\xe2\\x80\\x99t hold with such \\nnonsense. \\n\\nMr. Dursley w'\n",
      "b'as the director of a firm called \\nGrunnings, which made drills. He was a big, beefy \\nman with hardly '\n",
      "b'any neck, although he did have a \\nvery large mustache. Mrs. Dursley was thin and \\nblonde and had near'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "wLSBRCdflyGE",
   "metadata": {
    "id": "wLSBRCdflyGE"
   },
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5VLq3rN8l2Si",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5VLq3rN8l2Si",
    "outputId": "b895e3b1-78fc-4e86-a133-ada679555934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)> 6397600 6461583\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "print(dataset, len(dataset) * seq_length, len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "kr2-ebUpl9Yi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kr2-ebUpl9Yi",
    "outputId": "329b75e0-69aa-406d-e5f9-d8079519fbd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'THE BOY WHO LIVED \\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, \\nwere proud to say that they'\n",
      "Target: b'HE BOY WHO LIVED \\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, \\nwere proud to say that they '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14qwMuN5l-M_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14qwMuN5l-M_",
    "outputId": "000c42d2-2005-43d7-99ef-1f58f1a3546b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((None, 100), (None, 100)), types: (tf.int64, tf.int64)> 6400000 6461583\n"
     ]
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=False)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "print(dataset, len(dataset) * seq_length * BATCH_SIZE, len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "EPj39r7S1Y2I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EPj39r7S1Y2I",
    "outputId": "ca60ef7f-c1dc-4f7b-c853-13038547f8de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to use 800 batches for training and 200 for validation\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SIZE = 1/5\n",
    "\n",
    "_ = int(np.ceil(len(dataset) * (1 - VALIDATION_SIZE)))\n",
    "dataset_train = dataset.take(_)\n",
    "dataset_val = dataset.skip(_)\n",
    "print(\"Going to use\", len(dataset_train), \"batches for training and\", len(dataset_val), \"for validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2AHcCuqHmIDP",
   "metadata": {
    "id": "2AHcCuqHmIDP"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1mhc5cwumMS_",
   "metadata": {
    "id": "1mhc5cwumMS_"
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units, rnn_type=tf.keras.layers.SimpleRNN):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn_layer = rnn_type(rnn_units, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.rnn_layer.get_initial_state(x)\n",
    "        x, *states = self.rnn_layer(x, initial_state=states, training=training)\n",
    "        # print(len(states), type(states))\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74ICBIM2MVKt",
   "metadata": {
    "id": "74ICBIM2MVKt"
   },
   "outputs": [],
   "source": [
    "class OwnAccuracy(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"accuracy\", **kwargs):\n",
    "        super(OwnAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.accuracy = self.add_weight(name=\"acc\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = np.argmax(y_pred.numpy(), axis=2)\n",
    "        y_true = y_true.numpy()\n",
    "        correct = np.sum(y_pred == y_true, axis=1)\n",
    "        self.accuracy.assign(np.mean(correct / y_true.shape[1]))\n",
    "\n",
    "    def result(self):\n",
    "        return self.accuracy\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.accuracy.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "RmBkHnYKmk1v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RmBkHnYKmk1v",
    "outputId": "d222c020-19f0-44b4-dc89-1f4a2c2f3d51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new model since there was no saved one\n",
      "----- GRU -----\n",
      "(64, 100, 93) (64, 100, 92)\n",
      "Prediction shape:  (64, 100, 93) (64, 100, 92)\n",
      "Mean loss:         4.53331\n",
      "93.06609 should be around or higher 92\n",
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  23808     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  95325     \n",
      "=================================================================\n",
      "Total params: 4,057,437\n",
      "Trainable params: 4,057,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "saved_models = sorted([x for x in (os.path.join(\"./\", y) for y in os.listdir(\"./\")) if os.path.basename(x).startswith(\"saved_model_\") and os.path.isdir(x)], \n",
    "                      key=lambda x: datetime.strptime(x.split(\"_\")[-1], '%Y%m%d%H%M%S'),\n",
    "                      reverse = True\n",
    ")\n",
    "\n",
    "SELECTED_ARCH = tf.keras.layers.GRU\n",
    "\n",
    "if len(saved_models) <= 0:\n",
    "    print(\"Creating new model since there was no saved one\")\n",
    "    model_use = MyModel(\n",
    "        # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "        vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "        embedding_dim=embedding_dim,\n",
    "        rnn_units=rnn_units,\n",
    "        rnn_type=SELECTED_ARCH\n",
    "        )\n",
    "\n",
    "    print(\"-\" * 5, SELECTED_ARCH.__name__, \"-\" * 5)\n",
    "    for input_example_batch, target_example_batch in dataset.take(1):\n",
    "        example_batch_predictions = model_use(input_example_batch)\n",
    "        print(example_batch_predictions.shape, (BATCH_SIZE, seq_length, vocab_size))\n",
    "        pass\n",
    "    \n",
    "    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "    mean_loss = example_batch_loss.numpy().mean()\n",
    "    print(\"Prediction shape: \", example_batch_predictions.shape, (BATCH_SIZE, seq_length, vocab_size))\n",
    "    print(\"Mean loss:        \", mean_loss)\n",
    "\n",
    "    print(tf.exp(mean_loss).numpy(), \"should be around or higher\", vocab_size)\n",
    "\n",
    "    model_use.compile(optimizer='adam', loss=loss, metrics=[OwnAccuracy()], run_eagerly=True)\n",
    "    \n",
    "else:\n",
    "    print(\"Loading saved model from\", saved_models[0])\n",
    "    model_use = keras.models.load_model(saved_models[0], custom_objects={\"OwnAccuracy\": OwnAccuracy})\n",
    "\n",
    "model_use.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "BDnYLa9Rocm0",
   "metadata": {
    "id": "BDnYLa9Rocm0"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "uOxIeAcmm5TT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOxIeAcmm5TT",
    "outputId": "a199b7d0-e6ba-4e58-84a1-f336e2528a22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " ss boy with her. \n",
      "\n",
      "“Whatever House I’m in, I hope she’s not in it,” said \n",
      "Ron. He threw his wand bac\n",
      "\n",
      "Next Char Predictions:\n",
      " jVNFc&jjZ]%f~H1X’c\"’ju,wl(lA-]h2AiE28C ;j'\\w>nsWf(?cOVfki2]w%U\n",
      "x6|i,fiaqxd:’-k|lCNFSh(y1□>E||9fZ8oT8\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy().decode(\"utf-8\"))\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "OSblB6DR1j24",
   "metadata": {
    "id": "OSblB6DR1j24"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rc8RGSdf1u87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rc8RGSdf1u87",
    "outputId": "d31fc6f5-099d-4de1-8954-3a2889a62080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "800/800 [==============================] - 115s 144ms/step - loss: 1.8740 - accuracy: 0.5931 - val_loss: 1.3616 - val_accuracy: 0.6015\n",
      "Epoch 2/300\n",
      "800/800 [==============================] - 115s 144ms/step - loss: 1.2596 - accuracy: 0.6172 - val_loss: 1.1952 - val_accuracy: 0.6480\n",
      "Epoch 3/300\n",
      "800/800 [==============================] - 115s 144ms/step - loss: 1.1555 - accuracy: 0.6420 - val_loss: 1.1362 - val_accuracy: 0.6678\n",
      "Epoch 4/300\n",
      "800/800 [==============================] - 115s 144ms/step - loss: 1.1027 - accuracy: 0.6606 - val_loss: 1.1040 - val_accuracy: 0.6492\n",
      "Epoch 5/300\n",
      "800/800 [==============================] - 115s 143ms/step - loss: 1.0669 - accuracy: 0.6756 - val_loss: 1.0777 - val_accuracy: 0.6697\n",
      "Epoch 6/300\n",
      "800/800 [==============================] - 115s 143ms/step - loss: 1.0395 - accuracy: 0.6687 - val_loss: 1.0660 - val_accuracy: 0.6655\n",
      "Epoch 7/300\n",
      "800/800 [==============================] - 115s 143ms/step - loss: 1.0166 - accuracy: 0.6803 - val_loss: 1.0535 - val_accuracy: 0.6795\n",
      "Epoch 8/300\n",
      "800/800 [==============================] - 114s 142ms/step - loss: 0.9968 - accuracy: 0.6612 - val_loss: 1.0452 - val_accuracy: 0.6770\n",
      "Epoch 9/300\n",
      "800/800 [==============================] - 115s 143ms/step - loss: 0.9801 - accuracy: 0.6819 - val_loss: 1.0369 - val_accuracy: 0.6777\n",
      "Epoch 10/300\n",
      "800/800 [==============================] - 114s 142ms/step - loss: 0.9654 - accuracy: 0.6834 - val_loss: 1.0337 - val_accuracy: 0.6955\n",
      "Epoch 11/300\n",
      "800/800 [==============================] - 114s 142ms/step - loss: 0.9535 - accuracy: 0.6952 - val_loss: 1.0332 - val_accuracy: 0.6990\n",
      "Epoch 12/300\n",
      "800/800 [==============================] - 114s 143ms/step - loss: 0.9440 - accuracy: 0.6750 - val_loss: 1.0290 - val_accuracy: 0.7028\n",
      "Epoch 13/300\n",
      "800/800 [==============================] - 114s 142ms/step - loss: 0.9358 - accuracy: 0.6805 - val_loss: 1.0283 - val_accuracy: 0.6970\n",
      "Epoch 14/300\n",
      "800/800 [==============================] - 115s 143ms/step - loss: 0.9293 - accuracy: 0.6839 - val_loss: 1.0289 - val_accuracy: 0.6830\n",
      "Epoch 15/300\n",
      "800/800 [==============================] - 116s 145ms/step - loss: 0.9255 - accuracy: 0.6692 - val_loss: 1.0298 - val_accuracy: 0.6867\n",
      "Epoch 16/300\n",
      "800/800 [==============================] - 116s 145ms/step - loss: 0.9220 - accuracy: 0.6759 - val_loss: 1.0312 - val_accuracy: 0.6805\n",
      "Epoch 17/300\n",
      "800/800 [==============================] - 116s 145ms/step - loss: 0.9197 - accuracy: 0.6869 - val_loss: 1.0302 - val_accuracy: 0.6737\n",
      "Epoch 18/300\n",
      "800/800 [==============================] - 114s 143ms/step - loss: 0.9185 - accuracy: 0.6747 - val_loss: 1.0353 - val_accuracy: 0.6925\n",
      "Epoch 19/300\n",
      "800/800 [==============================] - 114s 142ms/step - loss: 0.9184 - accuracy: 0.6853 - val_loss: 1.0368 - val_accuracy: 0.6750\n",
      "Epoch 20/300\n",
      "800/800 [==============================] - 114s 142ms/step - loss: 0.9199 - accuracy: 0.6675 - val_loss: 1.0361 - val_accuracy: 0.6795\n",
      "Epoch 21/300\n",
      "800/800 [==============================] - 114s 143ms/step - loss: 0.9208 - accuracy: 0.6697 - val_loss: 1.0398 - val_accuracy: 0.6982\n",
      "Epoch 22/300\n",
      "800/800 [==============================] - 114s 143ms/step - loss: 0.9241 - accuracy: 0.6861 - val_loss: 1.0434 - val_accuracy: 0.6925\n",
      "Epoch 23/300\n",
      "800/800 [==============================] - 114s 143ms/step - loss: 0.9263 - accuracy: 0.6852 - val_loss: 1.0443 - val_accuracy: 0.6982\n",
      "Epoch 24/300\n",
      "800/800 [==============================] - 115s 143ms/step - loss: 0.9291 - accuracy: 0.6869 - val_loss: 1.0463 - val_accuracy: 0.6800\n",
      "Epoch 25/300\n",
      "800/800 [==============================] - 114s 143ms/step - loss: 0.9332 - accuracy: 0.6677 - val_loss: 1.0479 - val_accuracy: 0.6880\n",
      "Epoch 26/300\n",
      "800/800 [==============================] - 115s 143ms/step - loss: 0.9393 - accuracy: 0.6720 - val_loss: 1.0510 - val_accuracy: 0.6680\n",
      "Epoch 27/300\n",
      "800/800 [==============================] - 114s 142ms/step - loss: 0.9449 - accuracy: 0.6761 - val_loss: 1.0546 - val_accuracy: 0.6762\n",
      "Epoch 28/300\n",
      "800/800 [==============================] - 114s 142ms/step - loss: 0.9499 - accuracy: 0.6642 - val_loss: 1.0555 - val_accuracy: 0.6730\n",
      "Epoch 29/300\n",
      "800/800 [==============================] - 114s 142ms/step - loss: 0.9554 - accuracy: 0.6677 - val_loss: 1.0616 - val_accuracy: 0.6760\n",
      "Epoch 30/300\n",
      "800/800 [==============================] - 114s 143ms/step - loss: 0.9633 - accuracy: 0.6737 - val_loss: 1.0682 - val_accuracy: 0.6775\n",
      "Epoch 31/300\n",
      "800/800 [==============================] - 114s 143ms/step - loss: 0.9660 - accuracy: 0.6645 - val_loss: 1.0661 - val_accuracy: 0.6743\n",
      "Epoch 32/300\n",
      "800/800 [==============================] - 114s 142ms/step - loss: 0.9738 - accuracy: 0.6661 - val_loss: 1.0740 - val_accuracy: 0.6480\n",
      "Epoch 33/300\n",
      "800/800 [==============================] - 115s 143ms/step - loss: 1.0059 - accuracy: 0.6583 - val_loss: 1.1345 - val_accuracy: 0.6447\n",
      "Epoch 34/300\n",
      "800/800 [==============================] - 114s 143ms/step - loss: 1.0486 - accuracy: 0.6533 - val_loss: 1.1094 - val_accuracy: 0.6727\n",
      "Epoch 35/300\n",
      "800/800 [==============================] - 115s 143ms/step - loss: 1.0387 - accuracy: 0.6656 - val_loss: 1.1056 - val_accuracy: 0.6610\n",
      "Epoch 36/300\n",
      "800/800 [==============================] - 114s 142ms/step - loss: 1.0320 - accuracy: 0.6573 - val_loss: 1.1063 - val_accuracy: 0.6650\n",
      "Epoch 37/300\n",
      "800/800 [==============================] - 114s 142ms/step - loss: 1.0383 - accuracy: 0.6452 - val_loss: 1.1214 - val_accuracy: 0.6543\n",
      "Epoch 38/300\n",
      "800/800 [==============================] - 114s 142ms/step - loss: 1.0497 - accuracy: 0.6597 - val_loss: 1.1179 - val_accuracy: 0.6392\n",
      "Epoch 39/300\n",
      "800/800 [==============================] - 115s 143ms/step - loss: 1.0679 - accuracy: 0.6553 - val_loss: 1.1394 - val_accuracy: 0.6488\n",
      "Epoch 40/300\n",
      "800/800 [==============================] - 115s 143ms/step - loss: 1.0903 - accuracy: 0.6339 - val_loss: 1.1795 - val_accuracy: 0.6538\n",
      "Epoch 41/300\n",
      "800/800 [==============================] - 115s 143ms/step - loss: 1.4067 - accuracy: 0.3400 - val_loss: 2.4848 - val_accuracy: 0.3245\n",
      "Epoch 42/300\n",
      "800/800 [==============================] - 115s 143ms/step - loss: 2.1651 - accuracy: 0.4053 - val_loss: 2.0749 - val_accuracy: 0.4017\n",
      "Epoch 43/300\n",
      "800/800 [==============================] - 115s 143ms/step - loss: 2.0741 - accuracy: 0.4059 - val_loss: 2.0545 - val_accuracy: 0.4020\n",
      "Epoch 44/300\n",
      "800/800 [==============================] - 114s 142ms/step - loss: 2.0677 - accuracy: 0.4081 - val_loss: 2.0797 - val_accuracy: 0.4045\n",
      "Epoch 45/300\n",
      "800/800 [==============================] - 115s 143ms/step - loss: 2.0683 - accuracy: 0.3963 - val_loss: 2.0531 - val_accuracy: 0.4067\n",
      "Epoch 46/300\n",
      "800/800 [==============================] - 114s 143ms/step - loss: 2.0989 - accuracy: 0.3975 - val_loss: 2.0941 - val_accuracy: 0.3960\n",
      "Epoch 47/300\n",
      "800/800 [==============================] - 114s 142ms/step - loss: 2.0628 - accuracy: 0.4112 - val_loss: 2.0361 - val_accuracy: 0.4175\n",
      "Epoch 48/300\n",
      "800/800 [==============================] - 114s 143ms/step - loss: 2.0577 - accuracy: 0.4050 - val_loss: 2.0636 - val_accuracy: 0.3988\n",
      "Epoch 49/300\n",
      "800/800 [==============================] - 115s 143ms/step - loss: 2.0779 - accuracy: 0.4087 - val_loss: 2.0588 - val_accuracy: 0.4045\n",
      "Epoch 50/300\n",
      "333/800 [===========>..................] - ETA: 59s - loss: 2.0382 - accuracy: 0.4156"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "history = model_use.fit(dataset_train, epochs=EPOCHS, validation_data=dataset_val)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Training took\", timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff7d67-c200-4f9b-a32a-257c3d1de287",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_use.save(f\"./saved_model_{datetime.now().strftime('%Y%m%d%H%M%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KFB05UJDIVh6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "KFB05UJDIVh6",
    "outputId": "8f4cc030-9de2-4b46-b850-6a2d72ed02c1"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.plot(history.history[\"loss\"], label='loss')\n",
    "ax.plot(history.history[\"accuracy\"], label='accuracy')\n",
    "ax.plot(history.history[\"val_loss\"], label='validation loss')\n",
    "ax.plot(history.history[\"val_accuracy\"], label='validation accuracy')\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.legend()\n",
    "\n",
    "fig.set_size_inches(20, 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7NtSxxQU13D_",
   "metadata": {
    "id": "7NtSxxQU13D_"
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())]\n",
    "        )\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "    \n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "    \n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y3rQDlh-3Mjo",
   "metadata": {
    "id": "Y3rQDlh-3Mjo"
   },
   "outputs": [],
   "source": [
    "one_step = OneStep(model_use, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vbz26RuX3cCh",
   "metadata": {
    "id": "Vbz26RuX3cCh"
   },
   "outputs": [],
   "source": [
    "def generate_text(initial_chars, amount_generation, one_step_model):\n",
    "    start = time.time()\n",
    "    states = None\n",
    "    next_char = tf.constant([initial_chars])\n",
    "    result = [next_char]\n",
    "\n",
    "    for n in range(amount_generation):\n",
    "        next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "        result.append(next_char)\n",
    "        if (n+1) % 10000 == 0:\n",
    "            print(\"Generation step\", n+1, \"/\", amount_generation)\n",
    "\n",
    "    result = tf.strings.join(result)\n",
    "    end = time.time()\n",
    "    return result[0].numpy().decode('utf-8'), end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jiULvQzK395q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jiULvQzK395q",
    "outputId": "7da95230-2900-4e8b-ad85-67fb2c4e5907"
   },
   "outputs": [],
   "source": [
    "generated_book_title = \"Harry Potter and Learned Machine\"\n",
    "generated_book_len = np.random.randint(min(len(x) for x in processed_texts.values()), max(len(x) for x in processed_texts.values()) + 1)\n",
    "generated_book_first = \"The redemption\\n\\n\".upper()\n",
    "\n",
    "generated_book_len = 20_000\n",
    "\n",
    "generated_text, t = generate_text(initial_chars=generated_book_first, amount_generation=generated_book_len, one_step_model=one_step)\n",
    "print(\"-\" * 25)\n",
    "save_book(len(books) + 1, generated_book_title, generated_text)\n",
    "print(generated_text[:1000])\n",
    "print(\"-\" * 25)\n",
    "print(\"Run time:\", timedelta(seconds=t))\n",
    "print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b9297c-359f-4716-9b49-a706829b4585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "TextPrediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
